#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Apr 29 13:19:10 2025

@author: gibaek
"""
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

import torch
from sklearn.model_selection import train_test_split

import os, sys
#%%
path_c = os.path.dirname(os.path.abspath(__file__))
path_p = os.path.dirname(path_c)
path_g = os.path.dirname(path_p)
root = os.path.dirname(path_g)

sys.path.append(path_c)

# CSV ÏùΩÍ∏∞
cap = 5
chb_num = '01'
model = 'AH'
df = pd.read_csv(f'{root}/data/cadence/chbmit/chb{chb_num}/sim_result/{model}_balance_cap_{cap}pF.csv')


# Í≥µÌÜµ ÏãúÍ∞ÑÏ∂ï
time = df['/Vout_FP2_F4 X'].values

# ÏÑ∏ Ï±ÑÎÑêÏùò voltage
voltages = df[['/Vout_FP2_F4 Y', '/Vout_FT10_T8 Y', '/Vout_T8_P8 Y']].values


# Spike detection: voltage <= -1VÏùº Îïå spike
spikes = (voltages <= -1.0).astype(int)  # (Ï†ÑÏ≤¥ÏÉòÌîåÏàò, 3)

X = []
y = []
total_samples = len(time)

start_idx = 0
while start_idx < total_samples:
    # ÌòÑÏû¨ windowÏùò ÏãúÏûë ÏãúÍ∞Ñ
    start_time = time[start_idx]
    end_time = start_time + 2  # 2Ï¥à later

    # start_idxÎ∂ÄÌÑ∞ end_time Ïù¥ÌïòÍπåÏßÄ Ìè¨Ìï®ÌïòÎäî indexÎì§ Ï∞æÍ∏∞
    window_mask = (time >= start_time) & (time < end_time)
    window_spikes = spikes[window_mask]

    # windowÍ∞Ä ÎπÑÏñ¥ÏûàÏúºÎ©¥ break
    if window_spikes.shape[0] == 0:
        break

    # firing rate Í≥ÑÏÇ∞
    firing_rate = window_spikes.sum(axis=0) / 2  # 2Ï¥à Í∏∞Ï§Ä

    # label Í≤∞Ï†ï: window Ï§ëÏã¨ ÏãúÍ∞Ñ
    center_time = (start_time + end_time) / 2
    label = 1 if center_time <= 446 else 0

    # Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
    X.append(firing_rate)
    y.append(label)

    # Îã§Ïùå windowÎ°ú Ïù¥Îèô: end_timeÏùÑ Ìè¨Ìï®ÌïòÎäî index Ï∞æÍ∏∞
    start_idx = window_mask.nonzero()[0][-1] + 1
    
# Î¶¨Ïä§Ìä∏Î•º ÌÖêÏÑúÎ°ú Î≥ÄÌôò
X = torch.tensor(X, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.long)

# train/val Î∂ÑÎ¶¨
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

X_train.shape, y_train.shape, X_val.shape, y_val.shape



import torch
import torch.nn as nn
import snntorch as snn

class FiringRateSNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(3, 6)  # input_dim=3, hidden_dim=32
        self.lif1 = snn.Leaky(beta=0.9)
        self.fc2 = nn.Linear(6, 2)  # hidden_dim=32, output_dim=2 (binary classification)

    def forward(self, x):
        # LIF Îâ¥Îü∞ ÏÉÅÌÉú Ï¥àÍ∏∞Ìôî
        mem1 = self.lif1.init_leaky()
        
        # Ï≤´ Î≤àÏß∏ layer
        cur1 = self.fc1(x)
        spk1, mem1 = self.lif1(cur1, mem1)
        
        # Ï∂úÎ†• layer (linear only)
        out = self.fc2(spk1)
        return out

from torch.utils.data import TensorDataset, DataLoader

# DataLoader Ï§ÄÎπÑ
batch_size = 8

train_dataset = TensorDataset(X_train, y_train)
val_dataset = TensorDataset(X_val, y_val)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)

# Î™®Îç∏, optimizer, loss ÏÑ§Ï†ï
model = FiringRateSNN()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
criterion = nn.CrossEntropyLoss()

# ÌïôÏäµ Î£®ÌîÑ
n_epochs = 200

for epoch in range(n_epochs):
    model.train()
    train_loss = 0
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    
    # Validation
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_X, batch_y in val_loader:
            outputs = model(batch_X)
            preds = outputs.argmax(dim=1)
            correct += (preds == batch_y).sum().item()
            total += batch_y.size(0)
    
    acc = correct / total
    print(f"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f} | Val Acc: {acc*100:.2f}%")

#%%

train = [X_train, y_train]
val = [X_val, y_val]
datas = [train, val]
preds = []
acts = []

for data in datas:
    pred = model(data[0]).detach().numpy()
    pred = np.argmax(pred, axis = 1)
    actual = data[1].numpy()
    preds.append(pred)
    acts.append(actual)

#%%
conf_train = confusion_matrix(acts[0], preds[0])
conf_test = confusion_matrix(acts[1], preds[1])


# visualization
fig, ax = plt.subplots(1,2,figsize = (16,6))
sns.heatmap(conf_train, annot = True, fmt = 'd', cmap = 'Reds', ax = ax[0])
sns.heatmap(conf_test, annot = True, fmt = 'd', cmap = 'Reds', ax = ax[1])

title = ['Train set', 'Validate set']
for ii in range(2):
    ax[ii].set_xlabel("Prediction")
    ax[ii].set_ylabel("Actual")
    ax[ii].set_title(f"SNN classification result {title[ii]}")

# Calculate evaluation metrics

    precision = precision_score(acts[ii], preds[ii], zero_division=0)
    recall = recall_score(acts[ii], preds[ii], zero_division=0)  # = sensitivity
    f1 = f1_score(acts[ii], preds[ii], zero_division=0)
    
    print(f"üîé Performance Metrics for measure {title[ii]}")
    print(f"Precision     : {precision:.3f}")
    print(f"Sensitivity   : {recall:.3f}")
    print(f"F1 Score      : {f1:.3f}\n")
